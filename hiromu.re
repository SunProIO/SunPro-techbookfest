= ディープラーニングでご飯を作ってみる

== はじめに

こんにちは。SunProメンバーのhiromu(@hiromu1996)です。
今回は、世間で何でもできると話題になっているディープラーニングについて、
本当は何でもできるのであれば、この空いたお腹も満たしてくれようということで、
ディープラーニングでご飯を作ってみるということに挑戦してみました。

...という出落ち記事です。
実際は、ディープラーニングを使った画像生成アルゴリズムとして
最近話題になっている「Deep Convolutional Generative Adversarial Network (DCGAN)」で
ご飯の画像を生成するということに挑戦してみました。

=== 対象について

この記事では、DCGANやその実装について詳しい解説をせず、
むしろ公開されている実装をベースにして話を進める予定です。
なので、この記事の対象は、以下の3種類の方となります。

 * DCGANについてざっくりと知りたい方
 * non-rootな計算環境でTensorFlowを使う方法を知りたい方
 * ご飯の画像データセットについて知りたい方

== DCGANとは

DCGANは、Generative Adversarial Network(GAN, 敵対的生成ネットワーク)を発展させたアルゴリズムです。
もともと、GANは2014年にGoodfellowらによって提案された@<bib>{goodfellow}もので、
2つのニューラルネットワークを競い合わせることで画像生成をさせるというアルゴリズムになっています。

=== GANのアルゴリズム

GANには、GeneratorとDiscriminatorという2つのニューラルネットワークが登場します。
Generatorは、ランダムなベクトルzから画像を生成するニューラルネットワークで、
Discriminatorは、サンプルデータ(学習データ)として与えられた画像とGeneratorが生成した画像を
見分けるニューラルネットワークとして学習されます。
ここで、DiscriminatorがGeneratorの生成した画像を見分けられるようになるほど、
GeneratorはDiscriminatorが見分けられないよう、リアルな画像を生成するようになるという仕組みです。

わかりやすく例えると、Generatorは贋作画家、Discriminatorは鑑定士といえます。
贋作画家には、鑑定士を騙せるような絵を書けると報酬が与えられ、
鑑定士は、贋作を見分けることができると報酬が与えられるという風に考えることができます。

//image[dcgan][GANのアルゴリズムのイメージ]

読者の中には、なぜ2つのニューラルネットワークを使うのかと疑問に思った方もいらっしゃるかもしれません。
Discriminatorがいなくても、Generatorがそれっぽい画像を生成した時に報酬を与えるようにすればいいようにも思えます。
しかしながら、「それっぽい」という判断を下すのが非常に難しいのです。
サンプルデータと似ているかという判定基準だと、サンプルデータによく似た画像を生成できるようにはなりますが、
全くのコピーしか生成されず、新しい画像を生成することができなくなります。
しかし、Discriminatorと競い合うという形式を取ることで、どういった特徴を持っていれば
サンプルデータの仲間として判断されやすいのかということを学習することができるようになるのです。

=== DCGANのアルゴリズム

GANの問題点として、Discriminatorが強くなりすぎてしまい、Generatorが上手くならないまま学習が進んでしまうということや、
Generatorがほとんど同じような画像しか生成しなくなってしまうということが往々にしてありました。

それを解決したのが、2015年にRadfordらによって提案されたDCGAN@<bib>{radford}です。
Radfordらは、GANのアルゴリズムに畳み込みニューラルネットワークに関する最新の研究成果を組み合わせることによって、
GeneratorとDiscriminatorをバランスよく、効果的に学習させることに成功しました。

GANとの違いを細かく説明しようとすると、畳み込みニューラルネットワークとは何かという点から詳しく述べる必要があるため、
本稿では割愛しますが、Springenbergら@<bib>{springenberg}に従ってプーリング層を畳み込み層で置き換えたり、全結合層を無くしたりしています。
また、最近のニューラルネットワークでは一般的になってきたテクニックではありますが、バッチ最適化を取り入れたり、
活性化関数にGeneratorではReLU(出力層ではTanh)を、DiscriminatorではLeakyReLUを用いたりしているようです。
